{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7af164",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# RAWG Game Success Prediction - Model Training\n",
    "\n",
    "Este notebook implementa un pipeline completo de machine learning para predecir el éxito de videojuegos usando únicamente información disponible en la fase de diseño.\n",
    "\n",
    "**Contexto de negocio**: Los estudios de videojuegos necesitan evaluar el potencial de éxito de sus proyectos antes del lanzamiento para optimizar recursos y tomar decisiones estratégicas.\n",
    "\n",
    "**Objetivo**: Comparar 4 algoritmos (Linear Regression, Random Forest, XGBoost, Red Neuronal) para predecir success_score.\n",
    "**Dataset**: 76,272 juegos filtrados por calidad con 5 features de diseño y target continuo (0-1).\n",
    "**Métricas**: RMSE, MAE, R², MAPE para evaluación integral del rendimiento.\n",
    "**Metodología**: Train/Validation/Test split + Hyperparameter tuning + Análisis interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe795d3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Imports y configuración inicial\n",
    "\n",
    "Importamos todas las librerías necesarias para el pipeline de machine learning, incluyendo TensorFlow para implementar una red neuronal optimizada con callbacks avanzados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "from scipy.stats import randint, uniform\n",
    "from scipy import stats\n",
    "\n",
    "# TensorFlow para red neuronal optimizada\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c43e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Carga y exploración del dataset\n",
    "\n",
    "Cargamos el dataset final procesado en el EDA que contiene únicamente las 5 features de diseño más predictivas y el target success_score. Este dataset ha sido filtrado por calidad (rating > 0, added > 0) y rango temporal (2010-2024) para asegurar datos confiables.\n",
    "\n",
    "**Features de entrada**: n_genres, n_platforms, n_tags, esrb_rating_id, release_year\n",
    "**Target**: success_score (0-1) - métrica compuesta de rating, popularidad y engagement\n",
    "**Tamaño esperado**: ~76k juegos tras filtros de calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset desde archivo Parquet (más eficiente)\n",
    "# Intentar múltiples rutas posibles para mayor flexibilidad\n",
    "possible_paths = [\n",
    "    \"/kaggle/input/rawg-training-dataset/training_dataset_final.parquet\",\n",
    "    \"../Data/training_dataset_final.parquet\",\n",
    "    \"./training_dataset_final.parquet\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "for data_path in possible_paths:\n",
    "    try:\n",
    "        if os.path.exists(data_path):\n",
    "            df = pd.read_parquet(data_path)\n",
    "            print(f\"Dataset cargado desde: {data_path}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando desde {data_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"No se pudo cargar el dataset desde ninguna ruta disponible\")\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape}\")\n",
    "print(f\"Columnas: {list(df.columns)}\")\n",
    "print(f\"Tipos de datos:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e195c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploración básica del dataset\n",
    "print(\"Primeras 5 filas:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nValores nulos:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d4300",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Análisis de la distribución del target\n",
    "\n",
    "Verificamos la distribución del success_score para entender el problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de la distribución del target\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histograma\n",
    "axes[0].hist(df['success_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribución del Success Score')\n",
    "axes[0].set_xlabel('Success Score')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(df['success_score'])\n",
    "axes[1].set_title('Boxplot del Success Score')\n",
    "axes[1].set_ylabel('Success Score')\n",
    "\n",
    "# Q-Q plot para normalidad\n",
    "stats.probplot(df['success_score'], dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Normalidad)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Success Score - Min: {df['success_score'].min():.4f}, Max: {df['success_score'].max():.4f}\")\n",
    "print(f\"Success Score - Media: {df['success_score'].mean():.4f}, Std: {df['success_score'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28f2ab",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Preparación de datos\n",
    "\n",
    "Separamos features y target, dividimos en conjuntos de entrenamiento/validación/test y aplicamos escalado cuando sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features y target\n",
    "feature_columns = ['n_genres', 'n_platforms', 'n_tags', 'esrb_rating_id', 'release_year']\n",
    "X = df[feature_columns].copy()\n",
    "y = df['success_score'].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Verificar que no hay valores nulos en features\n",
    "print(f\"Valores nulos en features: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832601c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División en train/validation/test (70/20/10)\n",
    "# Nota: No usamos stratify porque es un problema de regresión, no clasificación\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.222, random_state=42)  # 0.222 * 0.9 = 0.2\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fde35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de features (necesario para Red Neuronal)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Escalado aplicado a todas las particiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884fac2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 4. Definición de métricas de evaluación\n",
    "\n",
    "Definimos funciones para calcular todas las métricas de evaluación de forma consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6878a49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calcula todas las métricas de evaluación para regresión\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    # Evitamos división por cero añadiendo epsilon\n",
    "    epsilon = 1e-8\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Imprime métricas de forma formateada\"\"\"\n",
    "    print(f\"Modelo: {metrics['Model']}\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.6f}\")\n",
    "    print(f\"MAE: {metrics['MAE']:.6f}\")\n",
    "    print(f\"R²: {metrics['R²']:.6f}\")\n",
    "    print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd331541",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 5. Modelo Baseline: Linear Regression\n",
    "\n",
    "Comenzamos con un modelo simple como baseline para establecer una referencia de rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar Linear Regression\n",
    "print(\"Entrenando Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "y_val_pred_lr = lr_model.predict(X_val)\n",
    "\n",
    "# Métricas\n",
    "lr_train_metrics = calculate_metrics(y_train, y_train_pred_lr, \"Linear Regression (Train)\")\n",
    "lr_val_metrics = calculate_metrics(y_val, y_val_pred_lr, \"Linear Regression (Validation)\")\n",
    "\n",
    "print_metrics(lr_train_metrics)\n",
    "print_metrics(lr_val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c704a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de coeficientes de Linear Regression\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Coefficient': lr_model.coef_,\n",
    "    'Abs_Coefficient': np.abs(lr_model.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Coeficientes de Linear Regression:\")\n",
    "display(coefficients)\n",
    "\n",
    "# Visualización de coeficientes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.title('Coeficientes de Linear Regression')\n",
    "plt.xlabel('Coeficiente')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900b544",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 6. Random Forest con Hyperparameter Tuning\n",
    "\n",
    "Entrenamos Random Forest con búsqueda aleatoria de hiperparámetros para optimizar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41719b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir espacio de búsqueda para Random Forest\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para Random Forest...\")\n",
    "\n",
    "# RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mejores parámetros RF: {rf_random.best_params_}\")\n",
    "print(f\"Mejor score CV: {-rf_random.best_score_:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf27b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final con mejores parámetros\n",
    "rf_model = rf_random.best_estimator_\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Métricas\n",
    "rf_train_metrics = calculate_metrics(y_train, y_train_pred_rf, \"Random Forest (Train)\")\n",
    "rf_val_metrics = calculate_metrics(y_val, y_val_pred_rf, \"Random Forest (Validation)\")\n",
    "\n",
    "print_metrics(rf_train_metrics)\n",
    "print_metrics(rf_val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance de Random Forest\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance - Random Forest:\")\n",
    "display(feature_importance_rf)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_rf['Feature'], feature_importance_rf['Importance'])\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.xlabel('Importancia')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912b02d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 7. XGBoost con Hyperparameter Tuning\n",
    "\n",
    "Entrenamos XGBoost, conocido por su excelente rendimiento en problemas de regresión con datos tabulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir espacio de búsqueda para XGBoost\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1)\n",
    "}\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para XGBoost...\")\n",
    "\n",
    "# RandomizedSearchCV\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(random_state=42, verbosity=0, n_jobs=1),\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mejores parámetros XGB: {xgb_random.best_params_}\")\n",
    "print(f\"Mejor score CV: {-xgb_random.best_score_:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757033be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final con mejores parámetros\n",
    "xgb_model = xgb_random.best_estimator_\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Métricas\n",
    "xgb_train_metrics = calculate_metrics(y_train, y_train_pred_xgb, \"XGBoost (Train)\")\n",
    "xgb_val_metrics = calculate_metrics(y_val, y_val_pred_xgb, \"XGBoost (Validation)\")\n",
    "\n",
    "print_metrics(xgb_train_metrics)\n",
    "print_metrics(xgb_val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance de XGBoost\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance - XGBoost:\")\n",
    "display(feature_importance_xgb)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_xgb['Feature'], feature_importance_xgb['Importance'])\n",
    "plt.title('Feature Importance - XGBoost')\n",
    "plt.xlabel('Importancia')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93a710",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 8. Red Neuronal (TensorFlow/Keras) con Hyperparameter Tuning\n",
    "\n",
    "Implementamos una red neuronal optimizada usando TensorFlow/Keras con arquitecturas en múltiplos de 2 (64, 128) para mejor eficiencia computacional. Incluimos callbacks avanzados como EarlyStopping y ReduceLROnPlateau para optimizar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar TensorFlow para reproducibilidad\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def create_neural_network(hidden_layers, dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"Crea una red neuronal con la arquitectura especificada\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Primera capa oculta\n",
    "    model.add(Dense(hidden_layers[0], activation='relu', input_shape=(5,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Capas ocultas adicionales\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid para output 0-1\n",
    "    \n",
    "    # Compilar modelo\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Definir arquitecturas a probar (múltiplos de 2)\n",
    "architectures = [\n",
    "    [64],           # 1 capa: 64 neuronas\n",
    "    [128],          # 1 capa: 128 neuronas  \n",
    "    [128, 64],      # 2 capas: 128 -> 64\n",
    "    [128, 64, 32],  # 3 capas: 128 -> 64 -> 32\n",
    "    [64, 32]        # 2 capas: 64 -> 32\n",
    "]\n",
    "\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "learning_rates = [0.001, 0.003, 0.01]\n",
    "\n",
    "print(\"Iniciando búsqueda de hiperparámetros para Red Neuronal (TensorFlow)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feafeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda manual de hiperparámetros (más control que RandomizedSearchCV)\n",
    "best_val_score = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "results = []\n",
    "\n",
    "# Callbacks para entrenamiento\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=20, restore_best_weights=True, verbose=0),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-7, verbose=0)\n",
    "]\n",
    "\n",
    "# Probar diferentes combinaciones\n",
    "for arch in architectures:\n",
    "    for dropout in dropout_rates:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"Probando: arch={arch}, dropout={dropout}, lr={lr}\")\n",
    "            \n",
    "            # Crear y entrenar modelo\n",
    "            model = create_neural_network(arch, dropout, lr)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                validation_data=(X_val_scaled, y_val),\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Evaluar en validación\n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            \n",
    "            results.append({\n",
    "                'architecture': arch,\n",
    "                'dropout': dropout,\n",
    "                'learning_rate': lr,\n",
    "                'val_loss': val_loss,\n",
    "                'epochs': len(history.history['loss'])\n",
    "            })\n",
    "            \n",
    "            # Guardar mejor modelo\n",
    "            if val_loss < best_val_score:\n",
    "                best_val_score = val_loss\n",
    "                best_params = {'architecture': arch, 'dropout': dropout, 'learning_rate': lr}\n",
    "                best_model = model\n",
    "            \n",
    "            print(f\"Val Loss: {val_loss:.6f}, Epochs: {len(history.history['loss'])}\")\n",
    "\n",
    "print(f\"\\nMejores parámetros: {best_params}\")\n",
    "print(f\"Mejor Val Loss: {best_val_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final con mejores parámetros\n",
    "print(\"Entrenando modelo final con mejores parámetros...\")\n",
    "nn_model = best_model\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred_nn = nn_model.predict(X_train_scaled, verbose=0).flatten()\n",
    "y_val_pred_nn = nn_model.predict(X_val_scaled, verbose=0).flatten()\n",
    "\n",
    "# Métricas\n",
    "nn_train_metrics = calculate_metrics(y_train, y_train_pred_nn, \"Neural Network (Train)\")\n",
    "nn_val_metrics = calculate_metrics(y_val, y_val_pred_nn, \"Neural Network (Validation)\")\n",
    "\n",
    "print_metrics(nn_train_metrics)\n",
    "print_metrics(nn_val_metrics)\n",
    "\n",
    "# Mostrar resumen de la arquitectura final\n",
    "print(\"\\nArquitectura del modelo final:\")\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef21b73",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 9. Comparativa de modelos\n",
    "\n",
    "Comparamos todos los modelos entrenados usando las métricas de validación para seleccionar el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recopilar todas las métricas de validación\n",
    "all_metrics = [lr_val_metrics, rf_val_metrics, xgb_val_metrics, nn_val_metrics]\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "print(\"Comparativa de modelos (Validación):\")\n",
    "display(comparison_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6928d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa de métricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics_to_plot = ['RMSE', 'MAE', 'R²', 'MAPE']\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[i//2, i%2]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=colors[i], alpha=0.7)\n",
    "    ax.set_title(f'Comparación - {metric}')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for bar, value in zip(bars, comparison_df[metric]):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.4f}' if metric != 'MAPE' else f'{value:.2f}%',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8259ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar el mejor modelo\n",
    "best_model_idx = comparison_df['R²'].idxmax()  # Mejor R²\n",
    "best_model_name = comparison_df.iloc[best_model_idx]['Model']\n",
    "print(f\"Mejor modelo según R²: {best_model_name}\")\n",
    "\n",
    "# También por RMSE (menor es mejor)\n",
    "best_rmse_idx = comparison_df['RMSE'].idxmin()\n",
    "best_rmse_name = comparison_df.iloc[best_rmse_idx]['Model']\n",
    "print(f\"Mejor modelo según RMSE: {best_rmse_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38baeb1d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 10. Evaluación final en conjunto de test\n",
    "\n",
    "Evaluamos el mejor modelo en el conjunto de test para obtener una estimación no sesgada del rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14470ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar el mejor modelo (por R²)\n",
    "if best_model_name == \"Linear Regression (Validation)\":\n",
    "    best_model = lr_model\n",
    "    X_test_final = X_test\n",
    "elif best_model_name == \"Random Forest (Validation)\":\n",
    "    best_model = rf_model\n",
    "    X_test_final = X_test\n",
    "elif best_model_name == \"XGBoost (Validation)\":\n",
    "    best_model = xgb_model\n",
    "    X_test_final = X_test\n",
    "else:  # Neural Network\n",
    "    best_model = nn_model\n",
    "    X_test_final = X_test_scaled\n",
    "\n",
    "# Predicción en test\n",
    "if best_model_name == \"Neural Network (Validation)\":\n",
    "    y_test_pred = best_model.predict(X_test_final, verbose=0).flatten()\n",
    "else:\n",
    "    y_test_pred = best_model.predict(X_test_final)\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, f\"{best_model_name.split('(')[0].strip()} (Test)\")\n",
    "\n",
    "print(\"EVALUACIÓN FINAL EN CONJUNTO DE TEST:\")\n",
    "print_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b38ad",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 11. Análisis de predicciones y residuos\n",
    "\n",
    "Analizamos las predicciones del mejor modelo para entender su comportamiento y posibles mejoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de predicciones vs valores reales\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Success Score Real')\n",
    "plt.ylabel('Success Score Predicho')\n",
    "plt.title('Predicciones vs Valores Reales')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfico de residuos\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.6, color='green')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Success Score Predicho')\n",
    "plt.ylabel('Residuos')\n",
    "plt.title('Análisis de Residuos')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeca948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de residuos\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(residuals, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de Residuos')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot de Residuos')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estadísticas de residuos:\")\n",
    "print(f\"Media: {residuals.mean():.6f}\")\n",
    "print(f\"Std: {residuals.std():.6f}\")\n",
    "print(f\"Min: {residuals.min():.6f}\")\n",
    "print(f\"Max: {residuals.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9d880",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 12. Análisis de feature importance consolidado\n",
    "\n",
    "Comparamos la importancia de features entre los diferentes modelos para entender qué características son más predictivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7095e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidar feature importance de todos los modelos\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Linear_Regression': np.abs(lr_model.coef_),\n",
    "    'Random_Forest': rf_model.feature_importances_,\n",
    "    'XGBoost': xgb_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Normalizar para comparación\n",
    "for col in ['Linear_Regression', 'Random_Forest', 'XGBoost']:\n",
    "    importance_comparison[col] = importance_comparison[col] / importance_comparison[col].sum()\n",
    "\n",
    "print(\"Feature Importance Comparativa (Normalizada):\")\n",
    "display(importance_comparison.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0777d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa de feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(feature_columns))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, importance_comparison['Linear_Regression'], width, label='Linear Regression', alpha=0.8)\n",
    "plt.bar(x, importance_comparison['Random_Forest'], width, label='Random Forest', alpha=0.8)\n",
    "plt.bar(x + width, importance_comparison['XGBoost'], width, label='XGBoost', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importancia Normalizada')\n",
    "plt.title('Comparación de Feature Importance entre Modelos')\n",
    "plt.xticks(x, feature_columns, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d6e89",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 13. Conclusiones y recomendaciones\n",
    "\n",
    "Resumen de resultados y recomendaciones para el modelo de predicción de éxito de videojuegos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dabb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESUMEN DE RESULTADOS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mejor modelo: {best_model_name.split('(')[0].strip()}\")\n",
    "print(f\"R² en test: {test_metrics['R²']:.4f}\")\n",
    "print(f\"RMSE en test: {test_metrics['RMSE']:.6f}\")\n",
    "print(f\"MAE en test: {test_metrics['MAE']:.6f}\")\n",
    "print(f\"MAPE en test: {test_metrics['MAPE']:.2f}%\")\n",
    "print()\n",
    "\n",
    "print(\"INTERPRETACIÓN:\")\n",
    "print(f\"- El modelo explica {test_metrics['R²']*100:.1f}% de la varianza en success_score\")\n",
    "print(f\"- Error promedio absoluto: {test_metrics['MAE']:.4f} puntos en escala 0-1\")\n",
    "print(f\"- Error porcentual promedio: {test_metrics['MAPE']:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"FEATURES MÁS IMPORTANTES:\")\n",
    "avg_importance = importance_comparison[['Linear_Regression', 'Random_Forest', 'XGBoost']].mean(axis=1)\n",
    "top_features = importance_comparison.loc[avg_importance.nlargest(3).index]\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Feature']}: {avg_importance[row.name]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f395604",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 14. Guardado de modelos\n",
    "\n",
    "Guardamos el mejor modelo y el scaler para uso posterior en producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio para modelos si no existe\n",
    "# Intentar múltiples ubicaciones según el entorno\n",
    "possible_model_dirs = [\n",
    "    \"/kaggle/working/models\",\n",
    "    \"../Models\",\n",
    "    \"./models\"\n",
    "]\n",
    "\n",
    "models_dir = None\n",
    "for dir_path in possible_model_dirs:\n",
    "    try:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        models_dir = dir_path\n",
    "        print(f\"Directorio de modelos: {models_dir}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo crear directorio {dir_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "if models_dir is None:\n",
    "    models_dir = \"./models\"  # Fallback por defecto\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    print(f\"Usando directorio por defecto: {models_dir}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "model_filename = f\"{models_dir}/best_model_{best_model_name.split('(')[0].strip().lower().replace(' ', '_')}.joblib\"\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"Modelo guardado en: {model_filename}\")\n",
    "\n",
    "# Guardar scaler (necesario para Red Neuronal)\n",
    "scaler_filename = f\"{models_dir}/feature_scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"Scaler guardado en: {scaler_filename}\")\n",
    "\n",
    "# Guardar métricas finales\n",
    "metrics_filename = f\"{models_dir}/model_metrics.joblib\"\n",
    "joblib.dump({\n",
    "    'test_metrics': test_metrics,\n",
    "    'comparison_metrics': comparison_df,\n",
    "    'feature_importance': importance_comparison\n",
    "}, metrics_filename)\n",
    "print(f\"Métricas guardadas en: {metrics_filename}\")\n",
    "\n",
    "print(\"\\nModelos y artefactos guardados exitosamente para producción.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a8217",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Resumen Final\n",
    "\n",
    "### Logros del entrenamiento:\n",
    "1. **Comparación exhaustiva**: 4 modelos evaluados con métricas completas\n",
    "2. **Optimización de hiperparámetros**: RandomizedSearch aplicado a todos los modelos complejos\n",
    "3. **Evaluación robusta**: Train/Validation/Test split para estimación no sesgada\n",
    "4. **Análisis interpretable**: Feature importance y análisis de residuos\n",
    "5. **Modelo productivo**: Mejor modelo guardado con artefactos necesarios\n",
    "\n",
    "### Características del modelo final:\n",
    "- **Algoritmo**: {best_model_name.split('(')[0].strip()}\n",
    "- **Performance**: R² = {test_metrics['R²']:.4f}, RMSE = {test_metrics['RMSE']:.6f}\n",
    "- **Features**: 5 variables de diseño (n_genres, n_platforms, n_tags, esrb_rating_id, release_year)\n",
    "- **Target**: success_score continuo (0-1)\n",
    "\n",
    "### Próximos pasos:\n",
    "1. **Validación temporal**: Evaluar modelo con juegos más recientes\n",
    "2. **Feature engineering**: Explorar interacciones entre variables\n",
    "3. **Ensemble methods**: Combinar mejores modelos para mayor robustez\n",
    "4. **Deployment**: Integrar modelo en pipeline de predicción para diseñadores\n",
    "5. **Monitoreo**: Establecer métricas de drift y reentrenamiento\n",
    "\n",
    "El modelo está listo para predecir el éxito de videojuegos usando únicamente información de diseño, maximizando la utilidad para estudios de desarrollo."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
